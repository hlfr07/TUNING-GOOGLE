{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.15384023691396484,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007692011845698242,
      "grad_norm": 5.549497127532959,
      "learning_rate": 9.993077455580341e-05,
      "loss": 2.9092,
      "step": 10
    },
    {
      "epoch": 0.0015384023691396484,
      "grad_norm": 1.976491928100586,
      "learning_rate": 9.985385739558496e-05,
      "loss": 2.1485,
      "step": 20
    },
    {
      "epoch": 0.002307603553709473,
      "grad_norm": 2.0198285579681396,
      "learning_rate": 9.977694023536651e-05,
      "loss": 2.0387,
      "step": 30
    },
    {
      "epoch": 0.003076804738279297,
      "grad_norm": 1.8230875730514526,
      "learning_rate": 9.970002307514807e-05,
      "loss": 1.878,
      "step": 40
    },
    {
      "epoch": 0.0038460059228491213,
      "grad_norm": 2.515336036682129,
      "learning_rate": 9.962310591492962e-05,
      "loss": 1.599,
      "step": 50
    },
    {
      "epoch": 0.004615207107418946,
      "grad_norm": 1.341552734375,
      "learning_rate": 9.954618875471117e-05,
      "loss": 1.5345,
      "step": 60
    },
    {
      "epoch": 0.005384408291988769,
      "grad_norm": 1.8878215551376343,
      "learning_rate": 9.946927159449274e-05,
      "loss": 1.54,
      "step": 70
    },
    {
      "epoch": 0.006153609476558594,
      "grad_norm": 2.6435387134552,
      "learning_rate": 9.939235443427429e-05,
      "loss": 1.6444,
      "step": 80
    },
    {
      "epoch": 0.006922810661128418,
      "grad_norm": 1.4245362281799316,
      "learning_rate": 9.931543727405584e-05,
      "loss": 1.6624,
      "step": 90
    },
    {
      "epoch": 0.007692011845698243,
      "grad_norm": 1.5932518243789673,
      "learning_rate": 9.92385201138374e-05,
      "loss": 1.5596,
      "step": 100
    },
    {
      "epoch": 0.008461213030268066,
      "grad_norm": 1.6297861337661743,
      "learning_rate": 9.916160295361895e-05,
      "loss": 1.7124,
      "step": 110
    },
    {
      "epoch": 0.009230414214837892,
      "grad_norm": 1.3576427698135376,
      "learning_rate": 9.908468579340052e-05,
      "loss": 1.4185,
      "step": 120
    },
    {
      "epoch": 0.009999615399407715,
      "grad_norm": 1.5302823781967163,
      "learning_rate": 9.900776863318207e-05,
      "loss": 1.4215,
      "step": 130
    },
    {
      "epoch": 0.010768816583977539,
      "grad_norm": 1.3594130277633667,
      "learning_rate": 9.893085147296363e-05,
      "loss": 1.6262,
      "step": 140
    },
    {
      "epoch": 0.011538017768547364,
      "grad_norm": 2.2779862880706787,
      "learning_rate": 9.885393431274518e-05,
      "loss": 1.6009,
      "step": 150
    },
    {
      "epoch": 0.012307218953117188,
      "grad_norm": 2.3005387783050537,
      "learning_rate": 9.877701715252673e-05,
      "loss": 1.5622,
      "step": 160
    },
    {
      "epoch": 0.013076420137687013,
      "grad_norm": 1.622051477432251,
      "learning_rate": 9.87000999923083e-05,
      "loss": 1.5065,
      "step": 170
    },
    {
      "epoch": 0.013845621322256836,
      "grad_norm": 2.561180353164673,
      "learning_rate": 9.862318283208985e-05,
      "loss": 1.4406,
      "step": 180
    },
    {
      "epoch": 0.01461482250682666,
      "grad_norm": 1.4396051168441772,
      "learning_rate": 9.85462656718714e-05,
      "loss": 1.703,
      "step": 190
    },
    {
      "epoch": 0.015384023691396485,
      "grad_norm": 1.9754945039749146,
      "learning_rate": 9.846934851165296e-05,
      "loss": 1.5299,
      "step": 200
    },
    {
      "epoch": 0.01615322487596631,
      "grad_norm": 1.6454581022262573,
      "learning_rate": 9.839243135143451e-05,
      "loss": 1.5173,
      "step": 210
    },
    {
      "epoch": 0.016922426060536132,
      "grad_norm": 2.2969982624053955,
      "learning_rate": 9.831551419121606e-05,
      "loss": 1.558,
      "step": 220
    },
    {
      "epoch": 0.017691627245105958,
      "grad_norm": 1.9236897230148315,
      "learning_rate": 9.823859703099763e-05,
      "loss": 1.568,
      "step": 230
    },
    {
      "epoch": 0.018460828429675783,
      "grad_norm": 1.619323492050171,
      "learning_rate": 9.816167987077918e-05,
      "loss": 1.5853,
      "step": 240
    },
    {
      "epoch": 0.019230029614245605,
      "grad_norm": 1.5518919229507446,
      "learning_rate": 9.808476271056073e-05,
      "loss": 1.5344,
      "step": 250
    },
    {
      "epoch": 0.01999923079881543,
      "grad_norm": 1.2078183889389038,
      "learning_rate": 9.800784555034228e-05,
      "loss": 1.4754,
      "step": 260
    },
    {
      "epoch": 0.020768431983385256,
      "grad_norm": 1.2427765130996704,
      "learning_rate": 9.793092839012384e-05,
      "loss": 1.3931,
      "step": 270
    },
    {
      "epoch": 0.021537633167955077,
      "grad_norm": 2.0345470905303955,
      "learning_rate": 9.78540112299054e-05,
      "loss": 1.6441,
      "step": 280
    },
    {
      "epoch": 0.022306834352524903,
      "grad_norm": 1.5890649557113647,
      "learning_rate": 9.777709406968695e-05,
      "loss": 1.4917,
      "step": 290
    },
    {
      "epoch": 0.023076035537094728,
      "grad_norm": 1.32297682762146,
      "learning_rate": 9.770017690946851e-05,
      "loss": 1.5072,
      "step": 300
    },
    {
      "epoch": 0.02384523672166455,
      "grad_norm": 1.6981974840164185,
      "learning_rate": 9.762325974925006e-05,
      "loss": 1.6156,
      "step": 310
    },
    {
      "epoch": 0.024614437906234375,
      "grad_norm": 2.6180286407470703,
      "learning_rate": 9.754634258903161e-05,
      "loss": 1.4748,
      "step": 320
    },
    {
      "epoch": 0.0253836390908042,
      "grad_norm": 1.3056936264038086,
      "learning_rate": 9.746942542881317e-05,
      "loss": 1.3891,
      "step": 330
    },
    {
      "epoch": 0.026152840275374026,
      "grad_norm": 1.2146891355514526,
      "learning_rate": 9.739250826859473e-05,
      "loss": 1.382,
      "step": 340
    },
    {
      "epoch": 0.026922041459943848,
      "grad_norm": 1.51837956905365,
      "learning_rate": 9.731559110837628e-05,
      "loss": 1.5079,
      "step": 350
    },
    {
      "epoch": 0.027691242644513673,
      "grad_norm": 1.1287418603897095,
      "learning_rate": 9.723867394815784e-05,
      "loss": 1.4713,
      "step": 360
    },
    {
      "epoch": 0.028460443829083498,
      "grad_norm": 1.1908934116363525,
      "learning_rate": 9.716175678793939e-05,
      "loss": 1.3834,
      "step": 370
    },
    {
      "epoch": 0.02922964501365332,
      "grad_norm": 1.4020377397537231,
      "learning_rate": 9.708483962772094e-05,
      "loss": 1.5461,
      "step": 380
    },
    {
      "epoch": 0.029998846198223145,
      "grad_norm": 1.1431522369384766,
      "learning_rate": 9.70079224675025e-05,
      "loss": 1.546,
      "step": 390
    },
    {
      "epoch": 0.03076804738279297,
      "grad_norm": 1.1929556131362915,
      "learning_rate": 9.693100530728406e-05,
      "loss": 1.4954,
      "step": 400
    },
    {
      "epoch": 0.03153724856736279,
      "grad_norm": 1.1795865297317505,
      "learning_rate": 9.685408814706562e-05,
      "loss": 1.417,
      "step": 410
    },
    {
      "epoch": 0.03230644975193262,
      "grad_norm": 1.5905719995498657,
      "learning_rate": 9.677717098684717e-05,
      "loss": 1.4782,
      "step": 420
    },
    {
      "epoch": 0.03307565093650244,
      "grad_norm": 1.2628378868103027,
      "learning_rate": 9.670025382662873e-05,
      "loss": 1.5496,
      "step": 430
    },
    {
      "epoch": 0.033844852121072265,
      "grad_norm": 1.628008484840393,
      "learning_rate": 9.662333666641029e-05,
      "loss": 1.4429,
      "step": 440
    },
    {
      "epoch": 0.034614053305642094,
      "grad_norm": 1.9187036752700806,
      "learning_rate": 9.654641950619184e-05,
      "loss": 1.4073,
      "step": 450
    },
    {
      "epoch": 0.035383254490211916,
      "grad_norm": 1.1944677829742432,
      "learning_rate": 9.64695023459734e-05,
      "loss": 1.4688,
      "step": 460
    },
    {
      "epoch": 0.03615245567478174,
      "grad_norm": 1.4174152612686157,
      "learning_rate": 9.639258518575495e-05,
      "loss": 1.4204,
      "step": 470
    },
    {
      "epoch": 0.036921656859351566,
      "grad_norm": 1.2962955236434937,
      "learning_rate": 9.63156680255365e-05,
      "loss": 1.4353,
      "step": 480
    },
    {
      "epoch": 0.03769085804392139,
      "grad_norm": 1.7008849382400513,
      "learning_rate": 9.623875086531807e-05,
      "loss": 1.4229,
      "step": 490
    },
    {
      "epoch": 0.03846005922849121,
      "grad_norm": 1.6578978300094604,
      "learning_rate": 9.616183370509962e-05,
      "loss": 1.368,
      "step": 500
    },
    {
      "epoch": 0.03922926041306104,
      "grad_norm": 1.5099139213562012,
      "learning_rate": 9.608491654488117e-05,
      "loss": 1.5001,
      "step": 510
    },
    {
      "epoch": 0.03999846159763086,
      "grad_norm": 1.4962353706359863,
      "learning_rate": 9.600799938466273e-05,
      "loss": 1.5654,
      "step": 520
    },
    {
      "epoch": 0.04076766278220068,
      "grad_norm": 1.8258227109909058,
      "learning_rate": 9.593108222444428e-05,
      "loss": 1.42,
      "step": 530
    },
    {
      "epoch": 0.04153686396677051,
      "grad_norm": 1.7531230449676514,
      "learning_rate": 9.585416506422583e-05,
      "loss": 1.5893,
      "step": 540
    },
    {
      "epoch": 0.04230606515134033,
      "grad_norm": 1.2774176597595215,
      "learning_rate": 9.577724790400738e-05,
      "loss": 1.3559,
      "step": 550
    },
    {
      "epoch": 0.043075266335910155,
      "grad_norm": 1.2576335668563843,
      "learning_rate": 9.570033074378895e-05,
      "loss": 1.4875,
      "step": 560
    },
    {
      "epoch": 0.043844467520479984,
      "grad_norm": 1.0111825466156006,
      "learning_rate": 9.56234135835705e-05,
      "loss": 1.5724,
      "step": 570
    },
    {
      "epoch": 0.044613668705049805,
      "grad_norm": 1.094400405883789,
      "learning_rate": 9.554649642335205e-05,
      "loss": 1.4534,
      "step": 580
    },
    {
      "epoch": 0.04538286988961963,
      "grad_norm": 1.3883867263793945,
      "learning_rate": 9.546957926313361e-05,
      "loss": 1.4771,
      "step": 590
    },
    {
      "epoch": 0.046152071074189456,
      "grad_norm": 1.105923056602478,
      "learning_rate": 9.539266210291516e-05,
      "loss": 1.4627,
      "step": 600
    },
    {
      "epoch": 0.04692127225875928,
      "grad_norm": 1.0335873365402222,
      "learning_rate": 9.531574494269671e-05,
      "loss": 1.437,
      "step": 610
    },
    {
      "epoch": 0.0476904734433291,
      "grad_norm": 1.5423599481582642,
      "learning_rate": 9.523882778247828e-05,
      "loss": 1.5337,
      "step": 620
    },
    {
      "epoch": 0.04845967462789893,
      "grad_norm": 1.0613884925842285,
      "learning_rate": 9.516191062225983e-05,
      "loss": 1.5756,
      "step": 630
    },
    {
      "epoch": 0.04922887581246875,
      "grad_norm": 1.2797824144363403,
      "learning_rate": 9.508499346204138e-05,
      "loss": 1.4517,
      "step": 640
    },
    {
      "epoch": 0.04999807699703857,
      "grad_norm": 1.782926321029663,
      "learning_rate": 9.500807630182294e-05,
      "loss": 1.372,
      "step": 650
    },
    {
      "epoch": 0.0507672781816084,
      "grad_norm": 1.2319118976593018,
      "learning_rate": 9.493115914160449e-05,
      "loss": 1.4435,
      "step": 660
    },
    {
      "epoch": 0.05153647936617822,
      "grad_norm": 1.3405944108963013,
      "learning_rate": 9.485424198138604e-05,
      "loss": 1.4504,
      "step": 670
    },
    {
      "epoch": 0.05230568055074805,
      "grad_norm": 1.1382899284362793,
      "learning_rate": 9.477732482116761e-05,
      "loss": 1.4466,
      "step": 680
    },
    {
      "epoch": 0.05307488173531787,
      "grad_norm": 2.193279981613159,
      "learning_rate": 9.470040766094916e-05,
      "loss": 1.5302,
      "step": 690
    },
    {
      "epoch": 0.053844082919887695,
      "grad_norm": 1.01395845413208,
      "learning_rate": 9.462349050073072e-05,
      "loss": 1.5097,
      "step": 700
    },
    {
      "epoch": 0.054613284104457524,
      "grad_norm": 1.8969000577926636,
      "learning_rate": 9.454657334051227e-05,
      "loss": 1.3966,
      "step": 710
    },
    {
      "epoch": 0.055382485289027346,
      "grad_norm": 2.480724811553955,
      "learning_rate": 9.446965618029384e-05,
      "loss": 1.4843,
      "step": 720
    },
    {
      "epoch": 0.05615168647359717,
      "grad_norm": 1.2251293659210205,
      "learning_rate": 9.439273902007539e-05,
      "loss": 1.5446,
      "step": 730
    },
    {
      "epoch": 0.056920887658166996,
      "grad_norm": 1.3425339460372925,
      "learning_rate": 9.431582185985694e-05,
      "loss": 1.5007,
      "step": 740
    },
    {
      "epoch": 0.05769008884273682,
      "grad_norm": 1.3022100925445557,
      "learning_rate": 9.42389046996385e-05,
      "loss": 1.5202,
      "step": 750
    },
    {
      "epoch": 0.05845929002730664,
      "grad_norm": 1.3453969955444336,
      "learning_rate": 9.416198753942005e-05,
      "loss": 1.4175,
      "step": 760
    },
    {
      "epoch": 0.05922849121187647,
      "grad_norm": 1.2048815488815308,
      "learning_rate": 9.40850703792016e-05,
      "loss": 1.4845,
      "step": 770
    },
    {
      "epoch": 0.05999769239644629,
      "grad_norm": 1.384421944618225,
      "learning_rate": 9.400815321898317e-05,
      "loss": 1.4065,
      "step": 780
    },
    {
      "epoch": 0.06076689358101611,
      "grad_norm": 1.4141443967819214,
      "learning_rate": 9.393123605876472e-05,
      "loss": 1.444,
      "step": 790
    },
    {
      "epoch": 0.06153609476558594,
      "grad_norm": 1.4620822668075562,
      "learning_rate": 9.385431889854627e-05,
      "loss": 1.3986,
      "step": 800
    },
    {
      "epoch": 0.06230529595015576,
      "grad_norm": 1.062298059463501,
      "learning_rate": 9.377740173832783e-05,
      "loss": 1.4327,
      "step": 810
    },
    {
      "epoch": 0.06307449713472559,
      "grad_norm": 1.388364553451538,
      "learning_rate": 9.370048457810938e-05,
      "loss": 1.5512,
      "step": 820
    },
    {
      "epoch": 0.06384369831929541,
      "grad_norm": 1.3834658861160278,
      "learning_rate": 9.362356741789093e-05,
      "loss": 1.363,
      "step": 830
    },
    {
      "epoch": 0.06461289950386524,
      "grad_norm": 0.9859372973442078,
      "learning_rate": 9.354665025767248e-05,
      "loss": 1.4407,
      "step": 840
    },
    {
      "epoch": 0.06538210068843506,
      "grad_norm": 1.0761334896087646,
      "learning_rate": 9.346973309745405e-05,
      "loss": 1.5382,
      "step": 850
    },
    {
      "epoch": 0.06615130187300489,
      "grad_norm": 1.4506508111953735,
      "learning_rate": 9.33928159372356e-05,
      "loss": 1.3917,
      "step": 860
    },
    {
      "epoch": 0.06692050305757472,
      "grad_norm": 1.3127844333648682,
      "learning_rate": 9.331589877701715e-05,
      "loss": 1.5,
      "step": 870
    },
    {
      "epoch": 0.06768970424214453,
      "grad_norm": 0.9796258807182312,
      "learning_rate": 9.323898161679871e-05,
      "loss": 1.5474,
      "step": 880
    },
    {
      "epoch": 0.06845890542671436,
      "grad_norm": 1.1409400701522827,
      "learning_rate": 9.316206445658026e-05,
      "loss": 1.4802,
      "step": 890
    },
    {
      "epoch": 0.06922810661128419,
      "grad_norm": 1.2982456684112549,
      "learning_rate": 9.308514729636181e-05,
      "loss": 1.4506,
      "step": 900
    },
    {
      "epoch": 0.069997307795854,
      "grad_norm": 1.908997654914856,
      "learning_rate": 9.300823013614338e-05,
      "loss": 1.5139,
      "step": 910
    },
    {
      "epoch": 0.07076650898042383,
      "grad_norm": 1.03109610080719,
      "learning_rate": 9.293131297592493e-05,
      "loss": 1.393,
      "step": 920
    },
    {
      "epoch": 0.07153571016499366,
      "grad_norm": 1.3010272979736328,
      "learning_rate": 9.285439581570648e-05,
      "loss": 1.4543,
      "step": 930
    },
    {
      "epoch": 0.07230491134956347,
      "grad_norm": 1.461521029472351,
      "learning_rate": 9.277747865548804e-05,
      "loss": 1.5101,
      "step": 940
    },
    {
      "epoch": 0.0730741125341333,
      "grad_norm": 1.1576764583587646,
      "learning_rate": 9.27005614952696e-05,
      "loss": 1.3465,
      "step": 950
    },
    {
      "epoch": 0.07384331371870313,
      "grad_norm": 1.405369520187378,
      "learning_rate": 9.262364433505115e-05,
      "loss": 1.4501,
      "step": 960
    },
    {
      "epoch": 0.07461251490327295,
      "grad_norm": 3.0758397579193115,
      "learning_rate": 9.254672717483271e-05,
      "loss": 1.5363,
      "step": 970
    },
    {
      "epoch": 0.07538171608784278,
      "grad_norm": 2.2157742977142334,
      "learning_rate": 9.246981001461426e-05,
      "loss": 1.5295,
      "step": 980
    },
    {
      "epoch": 0.0761509172724126,
      "grad_norm": 1.0213663578033447,
      "learning_rate": 9.239289285439582e-05,
      "loss": 1.3471,
      "step": 990
    },
    {
      "epoch": 0.07692011845698242,
      "grad_norm": 1.5189999341964722,
      "learning_rate": 9.231597569417737e-05,
      "loss": 1.3922,
      "step": 1000
    },
    {
      "epoch": 0.07768931964155225,
      "grad_norm": 2.404869318008423,
      "learning_rate": 9.223905853395894e-05,
      "loss": 1.4131,
      "step": 1010
    },
    {
      "epoch": 0.07845852082612208,
      "grad_norm": 1.416405439376831,
      "learning_rate": 9.216214137374049e-05,
      "loss": 1.4976,
      "step": 1020
    },
    {
      "epoch": 0.07922772201069189,
      "grad_norm": 1.1192766427993774,
      "learning_rate": 9.208522421352204e-05,
      "loss": 1.5333,
      "step": 1030
    },
    {
      "epoch": 0.07999692319526172,
      "grad_norm": 1.4859908819198608,
      "learning_rate": 9.20083070533036e-05,
      "loss": 1.3879,
      "step": 1040
    },
    {
      "epoch": 0.08076612437983155,
      "grad_norm": 1.1376545429229736,
      "learning_rate": 9.193138989308515e-05,
      "loss": 1.5526,
      "step": 1050
    },
    {
      "epoch": 0.08153532556440136,
      "grad_norm": 1.228388786315918,
      "learning_rate": 9.18544727328667e-05,
      "loss": 1.5385,
      "step": 1060
    },
    {
      "epoch": 0.0823045267489712,
      "grad_norm": 1.6157920360565186,
      "learning_rate": 9.177755557264827e-05,
      "loss": 1.4349,
      "step": 1070
    },
    {
      "epoch": 0.08307372793354102,
      "grad_norm": 1.4265347719192505,
      "learning_rate": 9.170063841242982e-05,
      "loss": 1.3704,
      "step": 1080
    },
    {
      "epoch": 0.08384292911811084,
      "grad_norm": 1.3371663093566895,
      "learning_rate": 9.162372125221137e-05,
      "loss": 1.4326,
      "step": 1090
    },
    {
      "epoch": 0.08461213030268067,
      "grad_norm": 1.0625014305114746,
      "learning_rate": 9.154680409199293e-05,
      "loss": 1.4986,
      "step": 1100
    },
    {
      "epoch": 0.0853813314872505,
      "grad_norm": 1.05265474319458,
      "learning_rate": 9.146988693177449e-05,
      "loss": 1.4473,
      "step": 1110
    },
    {
      "epoch": 0.08615053267182031,
      "grad_norm": 1.4022202491760254,
      "learning_rate": 9.139296977155604e-05,
      "loss": 1.4244,
      "step": 1120
    },
    {
      "epoch": 0.08691973385639014,
      "grad_norm": 1.3479207754135132,
      "learning_rate": 9.13160526113376e-05,
      "loss": 1.5003,
      "step": 1130
    },
    {
      "epoch": 0.08768893504095997,
      "grad_norm": 1.3670779466629028,
      "learning_rate": 9.123913545111915e-05,
      "loss": 1.4374,
      "step": 1140
    },
    {
      "epoch": 0.08845813622552978,
      "grad_norm": 1.195002794265747,
      "learning_rate": 9.11622182909007e-05,
      "loss": 1.4632,
      "step": 1150
    },
    {
      "epoch": 0.08922733741009961,
      "grad_norm": 1.0970698595046997,
      "learning_rate": 9.108530113068225e-05,
      "loss": 1.491,
      "step": 1160
    },
    {
      "epoch": 0.08999653859466944,
      "grad_norm": 1.3139338493347168,
      "learning_rate": 9.100838397046382e-05,
      "loss": 1.4741,
      "step": 1170
    },
    {
      "epoch": 0.09076573977923925,
      "grad_norm": 1.4408226013183594,
      "learning_rate": 9.093146681024537e-05,
      "loss": 1.4047,
      "step": 1180
    },
    {
      "epoch": 0.09153494096380908,
      "grad_norm": 1.5214600563049316,
      "learning_rate": 9.085454965002692e-05,
      "loss": 1.4343,
      "step": 1190
    },
    {
      "epoch": 0.09230414214837891,
      "grad_norm": 1.4100128412246704,
      "learning_rate": 9.077763248980848e-05,
      "loss": 1.5578,
      "step": 1200
    },
    {
      "epoch": 0.09307334333294873,
      "grad_norm": 1.2034178972244263,
      "learning_rate": 9.070071532959003e-05,
      "loss": 1.3356,
      "step": 1210
    },
    {
      "epoch": 0.09384254451751856,
      "grad_norm": 1.3693609237670898,
      "learning_rate": 9.062379816937158e-05,
      "loss": 1.6274,
      "step": 1220
    },
    {
      "epoch": 0.09461174570208838,
      "grad_norm": 1.501070499420166,
      "learning_rate": 9.054688100915315e-05,
      "loss": 1.4378,
      "step": 1230
    },
    {
      "epoch": 0.0953809468866582,
      "grad_norm": 1.3853563070297241,
      "learning_rate": 9.04699638489347e-05,
      "loss": 1.3018,
      "step": 1240
    },
    {
      "epoch": 0.09615014807122803,
      "grad_norm": 1.4267091751098633,
      "learning_rate": 9.039304668871625e-05,
      "loss": 1.5378,
      "step": 1250
    },
    {
      "epoch": 0.09691934925579786,
      "grad_norm": 1.0411566495895386,
      "learning_rate": 9.031612952849781e-05,
      "loss": 1.5048,
      "step": 1260
    },
    {
      "epoch": 0.09768855044036767,
      "grad_norm": 1.220056176185608,
      "learning_rate": 9.023921236827936e-05,
      "loss": 1.4642,
      "step": 1270
    },
    {
      "epoch": 0.0984577516249375,
      "grad_norm": 1.2464909553527832,
      "learning_rate": 9.016229520806093e-05,
      "loss": 1.4619,
      "step": 1280
    },
    {
      "epoch": 0.09922695280950733,
      "grad_norm": 1.5272302627563477,
      "learning_rate": 9.008537804784248e-05,
      "loss": 1.4942,
      "step": 1290
    },
    {
      "epoch": 0.09999615399407714,
      "grad_norm": 1.1124026775360107,
      "learning_rate": 9.000846088762404e-05,
      "loss": 1.3812,
      "step": 1300
    },
    {
      "epoch": 0.10076535517864697,
      "grad_norm": 3.6025283336639404,
      "learning_rate": 8.993154372740559e-05,
      "loss": 1.3959,
      "step": 1310
    },
    {
      "epoch": 0.1015345563632168,
      "grad_norm": 1.0019656419754028,
      "learning_rate": 8.985462656718714e-05,
      "loss": 1.5957,
      "step": 1320
    },
    {
      "epoch": 0.10230375754778662,
      "grad_norm": 1.0429041385650635,
      "learning_rate": 8.97777094069687e-05,
      "loss": 1.4508,
      "step": 1330
    },
    {
      "epoch": 0.10307295873235645,
      "grad_norm": 1.0724204778671265,
      "learning_rate": 8.970079224675026e-05,
      "loss": 1.5461,
      "step": 1340
    },
    {
      "epoch": 0.10384215991692627,
      "grad_norm": 1.0427802801132202,
      "learning_rate": 8.962387508653181e-05,
      "loss": 1.4702,
      "step": 1350
    },
    {
      "epoch": 0.1046113611014961,
      "grad_norm": 1.1946699619293213,
      "learning_rate": 8.954695792631337e-05,
      "loss": 1.4675,
      "step": 1360
    },
    {
      "epoch": 0.10538056228606592,
      "grad_norm": 1.8650832176208496,
      "learning_rate": 8.947004076609492e-05,
      "loss": 1.2643,
      "step": 1370
    },
    {
      "epoch": 0.10614976347063575,
      "grad_norm": 1.251216173171997,
      "learning_rate": 8.939312360587647e-05,
      "loss": 1.6055,
      "step": 1380
    },
    {
      "epoch": 0.10691896465520558,
      "grad_norm": 1.1667182445526123,
      "learning_rate": 8.931620644565804e-05,
      "loss": 1.4909,
      "step": 1390
    },
    {
      "epoch": 0.10768816583977539,
      "grad_norm": 1.2923699617385864,
      "learning_rate": 8.923928928543959e-05,
      "loss": 1.5279,
      "step": 1400
    },
    {
      "epoch": 0.10845736702434522,
      "grad_norm": 1.126558780670166,
      "learning_rate": 8.916237212522114e-05,
      "loss": 1.3084,
      "step": 1410
    },
    {
      "epoch": 0.10922656820891505,
      "grad_norm": 1.1879762411117554,
      "learning_rate": 8.90854549650027e-05,
      "loss": 1.5635,
      "step": 1420
    },
    {
      "epoch": 0.10999576939348486,
      "grad_norm": 1.0664807558059692,
      "learning_rate": 8.900853780478425e-05,
      "loss": 1.4172,
      "step": 1430
    },
    {
      "epoch": 0.11076497057805469,
      "grad_norm": 1.3726905584335327,
      "learning_rate": 8.89316206445658e-05,
      "loss": 1.4325,
      "step": 1440
    },
    {
      "epoch": 0.11153417176262452,
      "grad_norm": 1.0854865312576294,
      "learning_rate": 8.885470348434735e-05,
      "loss": 1.3188,
      "step": 1450
    },
    {
      "epoch": 0.11230337294719434,
      "grad_norm": 1.2859562635421753,
      "learning_rate": 8.877778632412892e-05,
      "loss": 1.4653,
      "step": 1460
    },
    {
      "epoch": 0.11307257413176416,
      "grad_norm": 1.5037195682525635,
      "learning_rate": 8.870086916391047e-05,
      "loss": 1.4343,
      "step": 1470
    },
    {
      "epoch": 0.11384177531633399,
      "grad_norm": 1.503273606300354,
      "learning_rate": 8.862395200369202e-05,
      "loss": 1.416,
      "step": 1480
    },
    {
      "epoch": 0.11461097650090381,
      "grad_norm": 1.3512119054794312,
      "learning_rate": 8.854703484347358e-05,
      "loss": 1.4413,
      "step": 1490
    },
    {
      "epoch": 0.11538017768547364,
      "grad_norm": 1.0137388706207275,
      "learning_rate": 8.847011768325513e-05,
      "loss": 1.3874,
      "step": 1500
    },
    {
      "epoch": 0.11614937887004347,
      "grad_norm": 1.0724855661392212,
      "learning_rate": 8.839320052303668e-05,
      "loss": 1.3956,
      "step": 1510
    },
    {
      "epoch": 0.11691858005461328,
      "grad_norm": 1.033547043800354,
      "learning_rate": 8.831628336281825e-05,
      "loss": 1.3609,
      "step": 1520
    },
    {
      "epoch": 0.11768778123918311,
      "grad_norm": 1.091668725013733,
      "learning_rate": 8.82393662025998e-05,
      "loss": 1.4742,
      "step": 1530
    },
    {
      "epoch": 0.11845698242375294,
      "grad_norm": 1.3197766542434692,
      "learning_rate": 8.816244904238135e-05,
      "loss": 1.4947,
      "step": 1540
    },
    {
      "epoch": 0.11922618360832275,
      "grad_norm": 1.9225590229034424,
      "learning_rate": 8.808553188216291e-05,
      "loss": 1.3287,
      "step": 1550
    },
    {
      "epoch": 0.11999538479289258,
      "grad_norm": 1.1933479309082031,
      "learning_rate": 8.800861472194446e-05,
      "loss": 1.4449,
      "step": 1560
    },
    {
      "epoch": 0.12076458597746241,
      "grad_norm": 1.4083141088485718,
      "learning_rate": 8.793169756172603e-05,
      "loss": 1.532,
      "step": 1570
    },
    {
      "epoch": 0.12153378716203223,
      "grad_norm": 1.7241246700286865,
      "learning_rate": 8.785478040150758e-05,
      "loss": 1.4509,
      "step": 1580
    },
    {
      "epoch": 0.12230298834660205,
      "grad_norm": 1.3100916147232056,
      "learning_rate": 8.777786324128914e-05,
      "loss": 1.4142,
      "step": 1590
    },
    {
      "epoch": 0.12307218953117188,
      "grad_norm": 1.5646051168441772,
      "learning_rate": 8.77009460810707e-05,
      "loss": 1.4577,
      "step": 1600
    },
    {
      "epoch": 0.1238413907157417,
      "grad_norm": 1.1112648248672485,
      "learning_rate": 8.762402892085224e-05,
      "loss": 1.4265,
      "step": 1610
    },
    {
      "epoch": 0.12461059190031153,
      "grad_norm": 0.895348846912384,
      "learning_rate": 8.754711176063381e-05,
      "loss": 1.4431,
      "step": 1620
    },
    {
      "epoch": 0.12537979308488134,
      "grad_norm": 1.8591206073760986,
      "learning_rate": 8.747019460041536e-05,
      "loss": 1.4902,
      "step": 1630
    },
    {
      "epoch": 0.12614899426945117,
      "grad_norm": 1.1783785820007324,
      "learning_rate": 8.739327744019691e-05,
      "loss": 1.515,
      "step": 1640
    },
    {
      "epoch": 0.126918195454021,
      "grad_norm": 1.0758295059204102,
      "learning_rate": 8.731636027997847e-05,
      "loss": 1.482,
      "step": 1650
    },
    {
      "epoch": 0.12768739663859083,
      "grad_norm": 1.154694676399231,
      "learning_rate": 8.723944311976002e-05,
      "loss": 1.3982,
      "step": 1660
    },
    {
      "epoch": 0.12845659782316066,
      "grad_norm": 0.900531530380249,
      "learning_rate": 8.716252595954158e-05,
      "loss": 1.2587,
      "step": 1670
    },
    {
      "epoch": 0.12922579900773049,
      "grad_norm": 1.3878041505813599,
      "learning_rate": 8.708560879932314e-05,
      "loss": 1.4149,
      "step": 1680
    },
    {
      "epoch": 0.1299950001923003,
      "grad_norm": 1.2938153743743896,
      "learning_rate": 8.700869163910469e-05,
      "loss": 1.4687,
      "step": 1690
    },
    {
      "epoch": 0.13076420137687012,
      "grad_norm": 1.2825286388397217,
      "learning_rate": 8.693177447888624e-05,
      "loss": 1.4608,
      "step": 1700
    },
    {
      "epoch": 0.13153340256143994,
      "grad_norm": 1.2796757221221924,
      "learning_rate": 8.68548573186678e-05,
      "loss": 1.4748,
      "step": 1710
    },
    {
      "epoch": 0.13230260374600977,
      "grad_norm": 1.5096083879470825,
      "learning_rate": 8.677794015844936e-05,
      "loss": 1.5606,
      "step": 1720
    },
    {
      "epoch": 0.1330718049305796,
      "grad_norm": 0.9995130896568298,
      "learning_rate": 8.67010229982309e-05,
      "loss": 1.2723,
      "step": 1730
    },
    {
      "epoch": 0.13384100611514943,
      "grad_norm": 1.0397675037384033,
      "learning_rate": 8.662410583801246e-05,
      "loss": 1.4173,
      "step": 1740
    },
    {
      "epoch": 0.13461020729971923,
      "grad_norm": 1.1387073993682861,
      "learning_rate": 8.654718867779402e-05,
      "loss": 1.4969,
      "step": 1750
    },
    {
      "epoch": 0.13537940848428906,
      "grad_norm": 2.2104427814483643,
      "learning_rate": 8.647027151757557e-05,
      "loss": 1.4219,
      "step": 1760
    },
    {
      "epoch": 0.1361486096688589,
      "grad_norm": 0.950567364692688,
      "learning_rate": 8.639335435735712e-05,
      "loss": 1.3237,
      "step": 1770
    },
    {
      "epoch": 0.13691781085342872,
      "grad_norm": 0.9478992819786072,
      "learning_rate": 8.631643719713869e-05,
      "loss": 1.3793,
      "step": 1780
    },
    {
      "epoch": 0.13768701203799855,
      "grad_norm": 1.083507776260376,
      "learning_rate": 8.623952003692024e-05,
      "loss": 1.4037,
      "step": 1790
    },
    {
      "epoch": 0.13845621322256838,
      "grad_norm": 0.9976344108581543,
      "learning_rate": 8.616260287670179e-05,
      "loss": 1.505,
      "step": 1800
    },
    {
      "epoch": 0.13922541440713818,
      "grad_norm": 1.4735358953475952,
      "learning_rate": 8.608568571648335e-05,
      "loss": 1.4022,
      "step": 1810
    },
    {
      "epoch": 0.139994615591708,
      "grad_norm": 1.2413660287857056,
      "learning_rate": 8.60087685562649e-05,
      "loss": 1.3348,
      "step": 1820
    },
    {
      "epoch": 0.14076381677627783,
      "grad_norm": 2.5108237266540527,
      "learning_rate": 8.593185139604645e-05,
      "loss": 1.4425,
      "step": 1830
    },
    {
      "epoch": 0.14153301796084766,
      "grad_norm": 1.2662447690963745,
      "learning_rate": 8.585493423582802e-05,
      "loss": 1.3692,
      "step": 1840
    },
    {
      "epoch": 0.1423022191454175,
      "grad_norm": 0.9876958131790161,
      "learning_rate": 8.577801707560957e-05,
      "loss": 1.3833,
      "step": 1850
    },
    {
      "epoch": 0.14307142032998732,
      "grad_norm": 1.1480557918548584,
      "learning_rate": 8.570109991539113e-05,
      "loss": 1.3947,
      "step": 1860
    },
    {
      "epoch": 0.14384062151455712,
      "grad_norm": 1.0135090351104736,
      "learning_rate": 8.562418275517268e-05,
      "loss": 1.4283,
      "step": 1870
    },
    {
      "epoch": 0.14460982269912695,
      "grad_norm": 1.4254436492919922,
      "learning_rate": 8.554726559495425e-05,
      "loss": 1.3493,
      "step": 1880
    },
    {
      "epoch": 0.14537902388369678,
      "grad_norm": 1.0285848379135132,
      "learning_rate": 8.54703484347358e-05,
      "loss": 1.4565,
      "step": 1890
    },
    {
      "epoch": 0.1461482250682666,
      "grad_norm": 1.402911901473999,
      "learning_rate": 8.539343127451735e-05,
      "loss": 1.4096,
      "step": 1900
    },
    {
      "epoch": 0.14691742625283644,
      "grad_norm": 1.012098789215088,
      "learning_rate": 8.531651411429891e-05,
      "loss": 1.4487,
      "step": 1910
    },
    {
      "epoch": 0.14768662743740626,
      "grad_norm": 1.8082432746887207,
      "learning_rate": 8.523959695408046e-05,
      "loss": 1.4463,
      "step": 1920
    },
    {
      "epoch": 0.14845582862197607,
      "grad_norm": 1.3823951482772827,
      "learning_rate": 8.516267979386201e-05,
      "loss": 1.4613,
      "step": 1930
    },
    {
      "epoch": 0.1492250298065459,
      "grad_norm": 1.6247085332870483,
      "learning_rate": 8.508576263364358e-05,
      "loss": 1.4298,
      "step": 1940
    },
    {
      "epoch": 0.14999423099111572,
      "grad_norm": 1.3377496004104614,
      "learning_rate": 8.500884547342513e-05,
      "loss": 1.3389,
      "step": 1950
    },
    {
      "epoch": 0.15076343217568555,
      "grad_norm": 0.9382430911064148,
      "learning_rate": 8.493192831320668e-05,
      "loss": 1.5159,
      "step": 1960
    },
    {
      "epoch": 0.15153263336025538,
      "grad_norm": 0.9083433151245117,
      "learning_rate": 8.485501115298824e-05,
      "loss": 1.4753,
      "step": 1970
    },
    {
      "epoch": 0.1523018345448252,
      "grad_norm": 1.5655838251113892,
      "learning_rate": 8.477809399276979e-05,
      "loss": 1.4656,
      "step": 1980
    },
    {
      "epoch": 0.153071035729395,
      "grad_norm": 0.9748291969299316,
      "learning_rate": 8.470117683255134e-05,
      "loss": 1.4123,
      "step": 1990
    },
    {
      "epoch": 0.15384023691396484,
      "grad_norm": 1.1544221639633179,
      "learning_rate": 8.46242596723329e-05,
      "loss": 1.357,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 13001,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2483731562496000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
